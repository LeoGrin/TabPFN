{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "{'recompute_attn': True}\n",
      "<module 'wandb' from '/home/parietal/lgrinszt/.local/miniconda3/envs/tab_pfn2/lib/python3.9/site-packages/wandb/__init__.py'>\n",
      "Using cuda:2 device\n",
      "Batch size: 1\n",
      "Using distributed training: False\n",
      "Using a Transformer with 25.82 M parameters\n"
     ]
    }
   ],
   "source": [
    "from tabpfn.scripts.transformer_prediction_interface import TabPFNClassifier\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer, PowerTransformer, RobustScaler\n",
    "import torch\n",
    "from torch.nn import MultiheadAttention\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "\n",
    "device = \"cuda:2\"\n",
    "\n",
    "model = TabPFNClassifier(device=device)\n",
    "module = model.model[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabpfn.utils import normalize_data, to_ranking_low_mem, remove_outliers\n",
    "from tabpfn.priors.utils import normalize_by_used_features_f\n",
    "\n",
    "normalize_with_test = True #TODO change\n",
    "#eval_position = 100\n",
    "normalize_to_ranking = False\n",
    "max_features=100\n",
    "normalize_with_sqrt = False\n",
    "\n",
    "def preprocess_input(eval_xs, eval_ys, categorical_feats, preprocess_transform):\n",
    "    import warnings\n",
    "\n",
    "    if eval_xs.shape[1] > 1:\n",
    "        raise Exception(\"Transforms only allow one batch dim - TODO\")\n",
    "    if preprocess_transform != 'none':\n",
    "        if preprocess_transform == 'power' or preprocess_transform == 'power_all':\n",
    "            pt = PowerTransformer(standardize=True)\n",
    "        elif preprocess_transform == 'quantile' or preprocess_transform == 'quantile_all':\n",
    "            pt = QuantileTransformer(output_distribution='normal')\n",
    "        elif preprocess_transform == 'robust' or preprocess_transform == 'robust_all':\n",
    "            pt = RobustScaler(unit_variance=True)\n",
    "\n",
    "    # eval_xs, eval_ys = normalize_data(eval_xs), normalize_data(eval_ys)\n",
    "    eval_xs = normalize_data(eval_xs, normalize_positions=-1 if normalize_with_test else eval_position)\n",
    "\n",
    "    # Removing empty features\n",
    "    eval_xs = eval_xs[:, 0, :]\n",
    "    sel = [len(torch.unique(eval_xs[0:eval_ys.shape[0], col])) > 1 for col in range(eval_xs.shape[1])]\n",
    "    eval_xs = eval_xs[:, sel]\n",
    "\n",
    "    warnings.simplefilter('error')\n",
    "    if preprocess_transform != 'none':\n",
    "        eval_xs = eval_xs.cpu().numpy()\n",
    "        feats = set(range(eval_xs.shape[1])) if 'all' in preprocess_transform else set(\n",
    "            range(eval_xs.shape[1])) - set(categorical_feats)\n",
    "        for col in feats:\n",
    "            try:\n",
    "                pt.fit(eval_xs[0:eval_position, col:col + 1])\n",
    "                trans = pt.transform(eval_xs[:, col:col + 1])\n",
    "                # print(scipy.stats.spearmanr(trans[~np.isnan(eval_xs[:, col:col+1])], eval_xs[:, col:col+1][~np.isnan(eval_xs[:, col:col+1])]))\n",
    "                eval_xs[:, col:col + 1] = trans\n",
    "            except:\n",
    "                pass\n",
    "        eval_xs = torch.tensor(eval_xs).float()\n",
    "    warnings.simplefilter('default')\n",
    "\n",
    "    eval_xs = eval_xs.unsqueeze(1)\n",
    "\n",
    "    # TODO: Cautian there is information leakage when to_ranking is used, we should not use it\n",
    "    eval_xs = remove_outliers(eval_xs, normalize_positions=-1 if normalize_with_test else eval_position) if not normalize_to_ranking else normalize_data(to_ranking_low_mem(eval_xs))\n",
    "    # Rescale X\n",
    "    eval_xs = normalize_by_used_features_f(eval_xs, eval_xs.shape[-1], max_features,\n",
    "                                            normalize_with_sqrt=normalize_with_sqrt)\n",
    "\n",
    "    return eval_xs.detach().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_rf(X, y, n_train):\n",
    "    # Move X and y to CPU\n",
    "    X = X.to(\"cpu\")\n",
    "    y = y.to(\"cpu\")\n",
    "    # check if regression or classification\n",
    "    if len(np.unique(y)) > len(y) / 3:\n",
    "        rf = RandomForestRegressor()\n",
    "        classif = False\n",
    "        print(\"Regression\")\n",
    "    else:\n",
    "        rf = RandomForestClassifier()\n",
    "        classif = True\n",
    "        print(\"Classification\")\n",
    "    X_train, X_test = X[:n_train], X[n_train:]\n",
    "    y_train, y_test = y[:n_train], y[n_train:]\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    if classif:\n",
    "        return accuracy_score(y_test, y_pred)\n",
    "    else:\n",
    "        return r2_score(y_test, y_pred)\n",
    "\n",
    "def compute_accuracy_tabpfn_original(X, y, n_train):\n",
    "    X = X.to(\"cpu\") #FIXME\n",
    "    y = y.to(\"cpu\")\n",
    "    X_train, X_test = X[:n_train], X[n_train:]\n",
    "    y_train, y_test = y[:n_train], y[n_train:]\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return accuracy_score(y_test, y_pred)\n",
    "\n",
    "def compute_accuracy_tabpfn(X, y, n_train, preprocess=False):\n",
    "    # reshape X to be 3D\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "    # reshape y to be 2D\n",
    "    y = y.reshape(y.shape[0], 1)\n",
    "    if preprocess:\n",
    "        X = preprocess_input(X, y, [], 'power')\n",
    "    X = X.to(device)\n",
    "    y_pred = torch.argmax(module((X, y), single_eval_pos=n_train, x_already_encoded=False), dim=-1).reshape(-1)\n",
    "    # Check the accuracy\n",
    "    print(accuracy_score(y[-len(y_pred):].cpu(), y_pred.cpu())) #FIXME\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_attention(module, X1, X2, y, n_train, preprocess=False, lr=0.01, n_epochs=100, train_tabpfn=False):\n",
    "    # reshape X to be 3D\n",
    "    X1 = X1.reshape(X1.shape[0], 1, X1.shape[1])\n",
    "    X2 = X2.reshape(X2.shape[0], 1, X2.shape[1])\n",
    "    # reshape y to be 2D\n",
    "    y = y.reshape(y.shape[0], 1)\n",
    "    if preprocess:\n",
    "        X1 = preprocess_input(X1, y, [], 'power')\n",
    "        X2 = preprocess_input(X2, y, [], 'power')\n",
    "\n",
    "    n = 512 # to be compatible with pretrained TabPFN\n",
    "    attention = MultiheadAttention(embed_dim=n, num_heads=1, dropout=0.0, device=device)\n",
    "    W_q = nn.Linear(n, n).to(device)\n",
    "    W_k = nn.Linear(n, n).to(device)\n",
    "    W_v = nn.Linear(n, n).to(device)\n",
    "    #W_o = nn.Linear(n, 2).to(device)\n",
    "    W_q.requires_grad = True\n",
    "    W_k.requires_grad = True\n",
    "    W_v.requires_grad = True\n",
    "    #W_o.requires_grad = True\n",
    "    train_size = n_train\n",
    "    #TODO think about this. Do I really need a train set?\n",
    "    val_size = (len(X1) - train_size) // 2\n",
    "    test_size = len(X1) - train_size - val_size\n",
    "    if train_tabpfn:\n",
    "        optimizer = optim.Adam([*W_q.parameters(), *W_k.parameters(), *W_v.parameters(), *module.parameters()], lr=lr)\n",
    "    else:\n",
    "        optimizer = optim.Adam([*W_q.parameters(), *W_k.parameters(), *W_v.parameters()], lr=lr)\n",
    "        with torch.no_grad():\n",
    "            input = module.encoder(X1)\n",
    "            input2 = module.encoder(X2)\n",
    "    for i in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        if train_tabpfn:\n",
    "            input = module.encoder(X1)\n",
    "            input2 = module.encoder(X2)\n",
    "        Q = W_q(input)\n",
    "        K = W_k(input2)\n",
    "        V = W_v(input2)\n",
    "        #full_input = input + attention(Q, K, V)[0]\n",
    "        full_input = input + attention(Q, K, V)[0]\n",
    "        y_pred = module((full_input, y), single_eval_pos=train_size, x_already_encoded=True).squeeze()\n",
    "        #y_pred = W_o(full_input).squeeze()\n",
    "        loss = torch.nn.functional.cross_entropy(y_pred[:val_size], y[-len(y_pred):-len(y_pred) + val_size].squeeze().long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 2 == 0:\n",
    "            print(loss)\n",
    "        if i % 10 == 0:\n",
    "            y_pred = torch.argmax(y_pred, dim=-1).reshape(-1)\n",
    "            print(\"Test accuracy:\")\n",
    "            print(\"Number of test samples: \", test_size)\n",
    "            print(accuracy_score(y[-len(y_pred) + val_size:].cpu(), y_pred[val_size:].cpu()))\n",
    "            print(\"Train accuracy:\")\n",
    "            #print(\"Number of train samples: \", train_size)\n",
    "            #print(accuracy_score(y[:train_size].cpu(), y_pred[:train_size].cpu()))\n",
    "            print(\"Val accuracy:\")\n",
    "            print(\"Number of val samples: \", val_size)\n",
    "            print(accuracy_score(y[-len(y_pred):-len(y_pred) + val_size].cpu(), y_pred[:val_size].cpu()))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 192])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# class PairwiseTransformer(nn.Module):\n",
    "#     def __init__(self, n_features, n_features_2, nhead, dim_feedforward):\n",
    "#         super(PairwiseTransformer, self).__init__()\n",
    "#         self.transformer = nn.Transformer(\n",
    "#             d_model=n_features + n_features_2,\n",
    "#             nhead=nhead,\n",
    "#             num_encoder_layers=1,\n",
    "#             num_decoder_layers=1,\n",
    "#             dim_feedforward=dim_feedforward,\n",
    "#         )\n",
    "#         self.fc = nn.Linear(n_features + n_features_2, 1)\n",
    "\n",
    "#     def forward(self, x1, x2):\n",
    "#         print(x1.shape)\n",
    "#         print(x2.shape)\n",
    "#         combined_input = torch.cat((x1, x2), dim=-1).unsqueeze(0)\n",
    "#         print(combined_input.shape)\n",
    "#         output = self.transformer(combined_input, combined_input)\n",
    "#         output = output.squeeze(0)\n",
    "#         score = self.fc(output)\n",
    "#         return score.squeeze(-1)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, classif=True):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.classif = classif\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        if self.classif:\n",
    "            out = F.softmax(out, dim=-1)\n",
    "        return out\n",
    "\n",
    "class PairwiseMLP(nn.Module):\n",
    "    def __init__(self, n_features, n_features_2, dim_feedforward):\n",
    "        super(PairwiseMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_features + n_features_2, dim_feedforward)\n",
    "        self.fc2 = nn.Linear(dim_feedforward, dim_feedforward)\n",
    "        self.fc3 = nn.Linear(dim_feedforward, 1)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        combined_input = torch.cat((x1, x2), dim=-1)\n",
    "        output = F.relu(self.fc1(combined_input))\n",
    "        output = F.relu(self.fc2(output))\n",
    "        score = self.fc3(output)\n",
    "        return score.squeeze(-1)\n",
    "    \n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, n_features, n_features_2, dim_feedforward):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.scorer = PairwiseMLP(n_features, n_features_2, dim_feedforward)\n",
    "\n",
    "    def forward(self, M1, M2):\n",
    "        scores = []\n",
    "        for i in range(M1.size(0)):\n",
    "                score = self.scorer(M1[i].repeat(M2.shape[0], 1), M2)\n",
    "                scores.append(score)\n",
    "        scores = torch.stack(scores).view(M1.size(0), M2.size(0))\n",
    "        # Normalize scores\n",
    "        scores = F.softmax(scores, dim=1)\n",
    "        M2_avg = scores @ M2\n",
    "        return torch.cat((M1, M2_avg), dim=1)\n",
    "\n",
    "# Example usage\n",
    "n_samples_batch, n_features = 32, 128\n",
    "n_samples_2, n_features_2 = 16, 64\n",
    "nhead = 8\n",
    "dim_feedforward = 256\n",
    "\n",
    "matching_model = CustomModel(n_features, n_features_2, dim_feedforward)\n",
    "\n",
    "M1 = torch.randn(n_samples_batch, n_features)\n",
    "M2 = torch.randn(n_samples_2, n_features_2)\n",
    "\n",
    "output = matching_model(M1, M2)\n",
    "print(output.shape)  # Expected output shape: (n_samples_batch, n_features + n_features_2)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss tensor(0.3938, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.5040, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.3403, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.4515, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.1962, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.3621, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.1768, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.3493, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.1566, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.3507, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.1624, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.3629, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.1582, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.3527, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.1529, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.3417, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.1486, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.3448, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.1609, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.3562, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.1454, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.3361, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.1500, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.3394, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.1488, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.3417, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.1398, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.3258, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.1389, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.3305, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.1432, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.3320, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.1445, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.3385, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.1384, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.3330, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.1348, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.3234, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.1373, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.3306, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.1409, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.3326, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.1426, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.3375, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.1381, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.3282, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.1377, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.3305, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.1307, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.3214, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.1341, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.3302, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.1398, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.3359, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.1338, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.3260, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.1290, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.3199, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.1286, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.3195, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.1227, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.3102, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.1168, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.3014, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.1247, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.3145, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.1137, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.2945, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.1214, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.3105, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.1184, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.3070, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.1140, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.3016, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.1089, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.2936, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.1086, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.2921, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.1220, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.3135, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.1002, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.2786, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.1119, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.3005, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.1022, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.2837, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.1063, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.2885, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.1001, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.2802, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0927, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.2654, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0925, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.2679, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0872, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.2585, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0856, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.2557, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0845, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.2562, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0797, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.2422, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0842, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.2531, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0769, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.2412, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0782, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.2458, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0702, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.2280, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0670, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.2236, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0716, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.2329, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0603, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.2083, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0665, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.2229, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0657, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.2214, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0563, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.2020, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0622, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.2147, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0577, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.2033, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0669, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.2232, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0542, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1954, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0515, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1889, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0520, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1949, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0544, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1948, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0508, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1922, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0531, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1895, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0507, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1913, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0471, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1803, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0468, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1808, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0491, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1864, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0459, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1789, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0498, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1933, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0469, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1765, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0506, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1892, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0482, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1800, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0464, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1724, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0432, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1740, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0450, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1769, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0436, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1718, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0385, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1609, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0424, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1710, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0451, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1738, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0412, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1673, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0380, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1579, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0449, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1715, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0378, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1643, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0436, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1737, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0394, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1596, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0427, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1718, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0414, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1711, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0404, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1668, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0392, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1677, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0404, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1666, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0413, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1691, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0401, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1658, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0393, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1636, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0406, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1667, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0395, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1698, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0409, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1673, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0330, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1494, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0399, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1653, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0338, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1542, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0349, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1560, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0377, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1584, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0405, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1676, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0435, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1699, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0331, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1496, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0351, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1594, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0346, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1516, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0320, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1510, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0356, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1590, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0339, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1571, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0345, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1519, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0334, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1546, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0349, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1570, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0356, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1569, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0298, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1437, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0371, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1553, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0303, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1436, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0232, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1277, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0306, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1464, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0325, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1491, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0323, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1472, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0299, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1419, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0305, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1471, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0348, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1523, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0307, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1453, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0295, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1402, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0290, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1432, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0276, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1395, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0321, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1487, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0286, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1360, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0294, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1438, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0259, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1350, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0302, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1428, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0270, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1354, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0280, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1397, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0302, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1447, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0276, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1411, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0255, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1334, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0261, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1367, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0309, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1424, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0252, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1336, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0249, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1340, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0252, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1310, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0261, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1332, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0246, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1327, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0233, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1317, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0267, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1340, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0311, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1377, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0248, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1287, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0213, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1202, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0232, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1281, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0218, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1249, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0245, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1298, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0253, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1334, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0236, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1267, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0230, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1229, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0244, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1282, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0262, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1288, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0246, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1288, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0201, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1229, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0204, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1177, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0200, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1171, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0231, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1251, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0171, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1115, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0196, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1177, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0186, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1155, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0189, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1136, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0207, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1178, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0186, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1139, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0201, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1153, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0185, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1122, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0165, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1071, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0170, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1076, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0172, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1127, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0197, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1141, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0157, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1069, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0177, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1090, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0210, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1200, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0182, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1101, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0156, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1046, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0158, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1041, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0176, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1115, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0162, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1038, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0162, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1048, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0153, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1020, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0137, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0991, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0157, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1031, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0142, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.1023, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0138, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0986, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0135, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0954, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0147, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.1002, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0128, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0965, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0137, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0962, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0133, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0946, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0126, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0932, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0153, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0974, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0137, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0940, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0116, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0904, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0126, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0932, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0121, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0918, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0112, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0876, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0111, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0866, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0115, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0879, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0104, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0832, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0115, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0891, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0114, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0873, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0100, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0810, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0112, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0871, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0105, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0841, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0127, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0915, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0112, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0846, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0105, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0835, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0104, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0836, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0101, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0821, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0097, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0807, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0098, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0824, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0094, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0780, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0090, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0783, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0094, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0796, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0090, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0759, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0084, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0742, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0089, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0755, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0092, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0764, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0104, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0812, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0088, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0088, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0766, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0087, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0750, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0093, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0775, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0078, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0713, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0074, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0701, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0076, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0709, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0071, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0686, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0076, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0695, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0071, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0678, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0073, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0682, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0068, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0657, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0073, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0675, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0075, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0674, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0071, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0676, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0064, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0663, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0073, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0678, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0067, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0673, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0071, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0666, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0075, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0688, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0065, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0646, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0074, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0685, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0066, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0664, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0077, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0707, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0062, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0637, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0061, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0623, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0060, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0636, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0051, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0575, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0057, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0604, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0056, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0602, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0059, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0620, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0062, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0628, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0055, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0606, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0052, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0594, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0062, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0625, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0057, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0603, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0056, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0591, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0054, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0593, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0058, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0609, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0063, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0622, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0054, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0590, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0047, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0540, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0053, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0573, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0057, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0592, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0048, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0556, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0047, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0549, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0049, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0566, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0054, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0590, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0054, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0582, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0055, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0589, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0049, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0551, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0051, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0555, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0051, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0555, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0047, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0549, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0047, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0549, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0046, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0540, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0045, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0534, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0047, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0526, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0049, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0552, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0040, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0497, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0048, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0549, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0057, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0562, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0045, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0539, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0042, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0529, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0049, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0563, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0044, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0533, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0046, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0533, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0052, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0558, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0044, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0525, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0047, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0532, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0047, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0536, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0039, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0500, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0047, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0543, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0047, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0559, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0045, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0526, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0051, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0558, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0044, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0523, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0039, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0515, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0046, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0531, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0047, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0529, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0044, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0520, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0041, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0504, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0046, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0532, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0041, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0511, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0040, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0494, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0040, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0492, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0042, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0509, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0042, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0516, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0045, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0518, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0040, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0505, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0038, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0485, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0041, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0506, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0043, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0516, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0050, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0533, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0039, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0491, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0035, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0473, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0039, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0490, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0043, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0534, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0040, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0490, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0035, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0466, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0485, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0039, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0485, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0472, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0038, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0494, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0040, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0497, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0038, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0491, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0036, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0478, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0035, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0450, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0036, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0468, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0038, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0488, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0038, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0484, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0035, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0469, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0465, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0029, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0427, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0473, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0032, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0449, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0038, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0475, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0030, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0439, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0034, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0459, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0036, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0463, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0467, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0038, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0483, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0033, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0453, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0038, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0488, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0035, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0458, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0036, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0467, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0033, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0444, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0030, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0434, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0029, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0422, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0033, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0460, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0034, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0449, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0028, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0428, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0033, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0444, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0034, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0465, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0031, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0437, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0034, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0467, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0031, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0437, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0030, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0425, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0034, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0455, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0028, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0415, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0034, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0453, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0034, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0447, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0033, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0450, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0035, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0452, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0031, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0428, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0028, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0405, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0032, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0439, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0031, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0418, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0031, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0432, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0031, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0434, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0032, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0442, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0031, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0448, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0029, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0422, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0031, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0435, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0029, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0418, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0029, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0425, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0030, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0429, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0030, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0419, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0026, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0403, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0027, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0413, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0029, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0428, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0030, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0425, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0027, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0409, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0029, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0428, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0030, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0424, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0033, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0436, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0029, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0416, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0026, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0405, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0031, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0433, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0034, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0449, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0031, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0443, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0029, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0422, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0028, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0408, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0028, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0409, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0027, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0405, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0029, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0419, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0028, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0421, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0027, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0399, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0027, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0403, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0028, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0420, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0026, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0404, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0022, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0369, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0028, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0414, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0025, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0396, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0025, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0387, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0027, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0400, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0027, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0393, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0360, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0024, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0383, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0026, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0405, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0027, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0404, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0036, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0462, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0027, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0405, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0028, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0424, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0026, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0399, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0024, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0386, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0024, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0382, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0026, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0389, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0026, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0400, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0026, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0399, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0027, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0398, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0025, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0394, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0027, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0407, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0026, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0399, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0025, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0389, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0377, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0022, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0364, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0025, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0393, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0373, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0025, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0391, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0025, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0392, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0025, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0384, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0381, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0025, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0399, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0025, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0387, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0025, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0393, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0371, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0382, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0024, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0388, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0024, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0377, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0364, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0381, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0024, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0386, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0022, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0376, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0367, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0356, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0365, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0374, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0376, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0375, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0360, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0025, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0394, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0352, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0373, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0022, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0369, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0022, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0375, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0349, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0383, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0353, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0350, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0347, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0351, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0356, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0368, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0350, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0376, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0348, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0347, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0353, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0337, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0338, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0358, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0370, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0358, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0364, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0324, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0351, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0351, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0349, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0361, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0348, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0369, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0344, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0351, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0348, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0353, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0351, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0333, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0336, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0329, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0325, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0352, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0341, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0345, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0348, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0339, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0337, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0354, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0323, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0347, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0349, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0313, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0340, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0315, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0331, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0326, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0339, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0322, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0318, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0336, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0336, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0331, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0323, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0336, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0325, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0334, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0337, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0370, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0309, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0352, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0323, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0316, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0325, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0350, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0313, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0331, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0328, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0308, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0333, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0325, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0330, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0352, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0334, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0315, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0330, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0321, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0320, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0333, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0317, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0287, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0319, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0311, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0317, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0303, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0296, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0314, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0308, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0320, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0319, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0285, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0309, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0335, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0302, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0293, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0294, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0264, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0298, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0292, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0305, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0312, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0296, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0301, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0288, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0307, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0297, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0295, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0296, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0270, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0306, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0312, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0298, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0293, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0276, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0317, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0304, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0315, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0298, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0289, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0305, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0261, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0286, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0267, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0302, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0282, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0289, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0321, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0295, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0289, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0290, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0265, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0278, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0282, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0289, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0291, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0275, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0253, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0275, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0289, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0289, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0283, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0278, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0307, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0284, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0288, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0275, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0283, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0291, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0286, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0270, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0279, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0274, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0274, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0266, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0282, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0274, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0272, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0281, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0292, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0269, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0277, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0285, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0261, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0271, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0290, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0283, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0275, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0276, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0274, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0275, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0273, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0259, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0292, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0262, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0277, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0283, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0255, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0268, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0275, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0271, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0258, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0260, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0256, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0269, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0260, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0255, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0271, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0260, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0268, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0258, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0263, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0254, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0247, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0262, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0239, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0257, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0235, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0256, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0272, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0256, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0257, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0253, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0264, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0259, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0251, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0255, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0258, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0265, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0268, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0257, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0235, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0251, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0247, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0246, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0238, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0240, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0258, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0243, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0263, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0246, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0252, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0247, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0240, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0245, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0240, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0251, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0237, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0249, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0233, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0237, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0250, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0251, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0246, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0235, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0236, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0250, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0233, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0247, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0230, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0236, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0230, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0239, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0221, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0227, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0244, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0243, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0245, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0240, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0232, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0237, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0250, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0232, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0214, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0234, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0225, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0244, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0228, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0229, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0231, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0232, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0221, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0229, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0234, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0239, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0232, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0232, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0232, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0242, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0229, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0241, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0243, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0229, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0215, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0237, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0226, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0222, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0231, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0223, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0234, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0225, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0231, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0221, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0236, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0232, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0227, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0226, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0229, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0232, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0220, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0224, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0235, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0225, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0215, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0217, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0201, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0216, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0224, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0221, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0217, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0220, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0234, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0226, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0231, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0225, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0219, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0216, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0216, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0207, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0216, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0219, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0209, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0219, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0210, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0223, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0218, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0216, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0209, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0215, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0205, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0214, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0208, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0218, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0206, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0219, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0219, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0214, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0207, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0210, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0204, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0217, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0219, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0214, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0200, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0213, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0196, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0212, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0209, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0215, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0206, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0209, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0198, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0215, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0214, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0210, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0215, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0204, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0202, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0202, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0211, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0204, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0232, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0208, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0225, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0207, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0209, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0207, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0209, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0207, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0200, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0199, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0213, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0203, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0208, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0204, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0197, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0196, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0212, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0203, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0192, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0206, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0210, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0206, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0202, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0190, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0187, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0203, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0202, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0201, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0199, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0191, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0191, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0195, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0200, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0192, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0195, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0196, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0201, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0188, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0197, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0188, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0216, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0199, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0192, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0185, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0198, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0199, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0212, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0193, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0196, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0189, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0203, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0191, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0170, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0198, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0186, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0192, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0185, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0188, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0196, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0194, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0189, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0198, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0181, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0190, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0194, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0199, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0193, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0185, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0179, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0187, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0176, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0185, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0201, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0181, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0200, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0182, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0183, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0174, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0185, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0188, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0193, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0185, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0183, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0180, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0171, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0185, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0179, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0178, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0178, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0188, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0182, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0189, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0176, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0186, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0182, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0187, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0194, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0181, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0186, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0181, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0174, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0193, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0181, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0186, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0180, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0173, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0194, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0177, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0177, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0179, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0178, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0171, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0173, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0186, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0184, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0176, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0174, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0173, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0169, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0184, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0195, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0178, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0184, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0183, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0182, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0180, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0176, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0171, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0165, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0168, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0162, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0172, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0184, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0167, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0173, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0177, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0172, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0173, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0167, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0177, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0156, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0178, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0177, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0177, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0184, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0171, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0168, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0171, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0178, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0167, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0164, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0169, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0177, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0173, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0163, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0176, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0174, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0162, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0166, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0172, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0176, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0173, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0162, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0164, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0159, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0169, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0181, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0172, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0163, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0170, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0169, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0174, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0161, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0164, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0177, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0165, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0166, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0165, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0183, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "abs tensor(0.0170, grad_fn=<MeanBackward0>)\n",
      "--------\n",
      "Test loss tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "Test abs tensor(0.0175, grad_fn=<MeanBackward0>)\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "# Try to train a PairwiseMLP to return X1[0] - X2[0]\n",
    "n_features = 10\n",
    "\n",
    "pairwise_model = PairwiseMLP(n_features, n_features, 256)\n",
    "optimizer = torch.optim.Adam(pairwise_model.parameters(), lr=0.0001)\n",
    "for i in range(5000):\n",
    "    X1 = torch.randn(1024, n_features)\n",
    "    X2 = torch.randn(1024, n_features)\n",
    "    # y = exponential decay of X1[0] - X2[0]\n",
    "    y = torch.exp(- (X1[:, 0] - X2[:, 0])**2)\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = pairwise_model(X1, X2)\n",
    "    loss = torch.nn.functional.mse_loss(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 10 == 0:\n",
    "        print(\"Loss\", loss)\n",
    "        print(\"abs\", (y_pred - y).abs().mean())\n",
    "        X1_test = torch.randn(256, n_features)\n",
    "        X2_test = torch.randn(256, n_features)\n",
    "        y_test =torch.exp(- (X1_test[:, 0] - X2_test[:, 0])**2)\n",
    "        y_pred_test = pairwise_model(X1_test, X2_test)\n",
    "        print(\"--------\")\n",
    "        print(\"Test loss\", torch.nn.functional.mse_loss(y_pred_test, y_test))\n",
    "        print(\"Test abs\", (y_pred_test - y_test).abs().mean())\n",
    "        print(\"--------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f5293bf7970>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "X1 = torch.randn(32, n_features)\n",
    "X2 = torch.randn(32, n_features)\n",
    "y = torch.exp(- (X1[:, 0] - X2[:, 0])**2)\n",
    "y_pred = pairwise_model(X1, X2)\n",
    "print(y_pred.shape)\n",
    "plt.scatter(torch.abs(X1[:, 0] - X2[:, 0]).detach(), y_pred.detach(), c=\"red\")\n",
    "plt.scatter(torch.abs(X1[:, 0] - X2[:, 0]).detach(), y.detach(), c=\"blue\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_matching(X1, X2, y, batch_size, preprocess=False, lr=0.01, n_epochs=100, dim_feedforward=256, train_prop=0.7):\n",
    "    # reshape X to be 3D\n",
    "    X1 = X1.reshape(X1.shape[0], 1, X1.shape[1])\n",
    "    X2 = X2.reshape(X2.shape[0], 1, X2.shape[1])\n",
    "    # reshape y to be 2D\n",
    "    y = y.reshape(y.shape[0], 1)\n",
    "    if preprocess:\n",
    "        X1 = preprocess_input(X1, y, [], 'power')\n",
    "        X2 = preprocess_input(X2, y, [], 'power')\n",
    "    # reshape X to be 2D\n",
    "    X1 = X1.reshape(X1.shape[0], X1.shape[2])\n",
    "    X2 = X2.reshape(X2.shape[0], X2.shape[2]).to(device)\n",
    "    # reshape y to be 1D\n",
    "    y = y.reshape(y.shape[0])\n",
    "    scorer = CustomModel(n_features=X1.shape[1], n_features_2=X2.shape[1], dim_feedforward=dim_feedforward)\n",
    "    n_classes = torch.unique(y).shape[0]\n",
    "    print(\"num classes\", n_classes)\n",
    "    predictor = MLP(input_size=X1.shape[1] + X2.shape[1], hidden_size=dim_feedforward, output_size=n_classes, classif=True)\n",
    "    # move to GPU\n",
    "    scorer = scorer.to(device)\n",
    "    predictor = predictor.to(device)\n",
    "    optimizer = optim.Adam([*scorer.parameters(), *predictor.parameters()], lr=lr)\n",
    "    # create a dataset\n",
    "    print(X1.shape)\n",
    "    print(y.shape)\n",
    "    X1_train = X1[:int(train_prop * len(X1))]\n",
    "    X1_test = X1[int(train_prop * len(X1)):]\n",
    "    y_train = y[:int(train_prop * len(y))]\n",
    "    y_test = y[int(train_prop * len(y)):]\n",
    "    dataset_train = torch.utils.data.TensorDataset(X1_train, y_train)\n",
    "    dataset_test = torch.utils.data.TensorDataset(X1_test, y_test)\n",
    "    # create a data loader\n",
    "    dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "    dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"Epoch\", epoch)\n",
    "        print(\"----------------\")\n",
    "        train_loss = 0\n",
    "        train_accuracy = 0\n",
    "        for X1_batch, y_batch in dataloader_train:\n",
    "            # move to GPU\n",
    "            X1_batch = X1_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            # get the predictions\n",
    "            output = scorer(X1_batch, X2)\n",
    "            output = predictor(output)\n",
    "            # calculate the loss\n",
    "            loss = loss_function(output, y_batch)\n",
    "            train_loss += loss.item()\n",
    "            accuracy = (output.argmax(dim=1) == y_batch).float().mean()\n",
    "            train_accuracy += accuracy\n",
    "            #print(\"Train loss\", loss.item())\n",
    "            # backprop\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        print(\"Train loss\", train_loss / len(dataloader_train))\n",
    "        print(\"Train accuracy\", train_accuracy / len(dataloader_train))\n",
    "        # calculate the accuracy\n",
    "        print(\"---------------\")\n",
    "        test_loss = 0\n",
    "        test_accuracy = 0\n",
    "        for X1_batch, y_batch in dataloader_test:\n",
    "            # move to GPU\n",
    "            X1_batch = X1_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            # Calculate the predictions\n",
    "            output = scorer(X1_batch, X2)\n",
    "            output = predictor(output)\n",
    "            # calculate the loss and accuracy\n",
    "            loss = loss_function(output, y_batch)\n",
    "            test_loss += loss.item()\n",
    "            # calculate the accuracy\n",
    "            y_pred = output.argmax(dim=1)\n",
    "            accuracy = (y_pred == y_batch).float().mean()\n",
    "            test_accuracy += accuracy\n",
    "        print(\"Test loss\", test_loss / len(dataloader_test))\n",
    "        print(\"Test accuracy\", test_accuracy / len(dataloader_test))\n",
    "        print(\"---------------\")\n",
    "    return scorer, predictor\n",
    "\n",
    "            \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a non linear function\n",
    "#f = torch.functional.F.relu\n",
    "#f = lambda x: x**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fake datasets\n",
    "# Create a first dataset with columns A, B\n",
    "#torch.manual_seed(20)\n",
    "dataset1 = np.random.rand(10000, 10)\n",
    "# bucketize the columns\n",
    "dataset1_bucketized = np.floor(dataset1 * 10)\n",
    "# assign a random value to each combination of bucketized values\n",
    "unique_values = np.unique(dataset1_bucketized, axis=0)\n",
    "# create a dictionary\n",
    "dict_values = {}\n",
    "for i in range(len(unique_values)):\n",
    "    key = tuple(unique_values[i])\n",
    "    dict_values[key] = torch.rand(1)\n",
    "\n",
    "# create y from the dictionary\n",
    "y = torch.zeros(dataset1.shape[0])\n",
    "for i in range(dataset1.shape[0]):\n",
    "    key = tuple(dataset1_bucketized[i])\n",
    "    y[i] = dict_values[key]\n",
    "\n",
    "# Create a second dataset with the unique values of the first dataset and y as the last column\n",
    "dataset2 = np.zeros((11, dataset1.shape[1] + 1))\n",
    "for i in range(10):\n",
    "    dataset2[:, i] = np.arange(dataset1.shape[1] + 1)\n",
    "dataset2[:, -1] = np.random.randin\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#dataset2.requires_grad = True\n",
    "#dataset1.requires_grad = True\n",
    "\n",
    "# Make it a classification problem\n",
    "#y = (y > np.median(y)).float()\n",
    "\n",
    "#print(np.unique(y, return_counts=True))\n",
    "\n",
    "# Move to GPU\n",
    "#dataset1 = dataset1.to(device)\n",
    "#dataset2 = dataset2.to(device)\n",
    "#y = y.to(device)\n",
    "# Shuffle dataset2\n",
    "#dataset2 = dataset2[torch.randperm(len(dataset2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = torch.randn(500, 10)\n",
    "X2 = torch.randn(3000, 10)\n",
    "\n",
    "# Create a random function with an MLP\n",
    "f = MLP(input_size=10, hidden_size=100, output_size=1, classif=False)\n",
    "y = f(X1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = []\n",
    "# pairwise_model = pairwise_model.to(device)\n",
    "# for i in range(dataset1.size(0)):\n",
    "#         for j in range(dataset2.size(0)):\n",
    "#             #score = pairwise_model(dataset1[i], dataset2[j])\n",
    "#             score = torch.exp(- (dataset1[i][0] -  dataset2[j][0])**2)\n",
    "#             scores.append(score)\n",
    "#         #score = pairwise_model(dataset1[i].repeat(dataset2.shape[0], 1), dataset2)\n",
    "#         #scores.append(score)\n",
    "# scores = torch.stack(scores).view(dataset1.size(0), dataset2.size(0))\n",
    "# print(scores.shape)\n",
    "# # find best match for each row\n",
    "# best_match = torch.argmax(scores, dim=1)\n",
    "# #best_matches = torch.argsort(scores, dim=1, descending=True)[:,:5]\n",
    "# print(best_match.shape)\n",
    "# # y_pred is the dim 1 of the best match\n",
    "# y_pred = dataset2[best_match, 1]\n",
    "# #y_pred = torch.mean(dataset2[best_matches, 1], dim=1)\n",
    "\n",
    "# # rmse\n",
    "# print(np.sqrt(np.mean((y_pred.cpu().numpy() - y.cpu().numpy())**2)))\n",
    "\n",
    "# plt.scatter(y_pred.cpu(), y.cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8780, -1.2207, -0.6406,  ..., -1.0992, -0.1290,  0.3034],\n",
       "        [-0.1706,  0.7942,  0.1720,  ..., -0.1920, -2.4479, -0.9503],\n",
       "        [-0.1181,  0.9089, -2.1577,  ..., -0.1876, -0.5218,  0.4820],\n",
       "        ...,\n",
       "        [ 0.3391, -0.7876, -1.4242,  ...,  1.8168, -0.3592, -0.1758],\n",
       "        [ 1.6243, -0.5512,  1.2396,  ..., -2.4787, -0.2446,  1.5140],\n",
       "        [ 0.0963,  1.0195,  0.2070,  ...,  0.2282,  0.0640,  1.1154]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Random Forest --\n",
      "Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2180/3987199982.py:16: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rf.fit(X_train, y_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with dataset1 0.5973345060904891\n"
     ]
    }
   ],
   "source": [
    "print(\"-- Random Forest --\")\n",
    "y = f(X2)\n",
    "print(\"Accuracy with dataset1\", compute_accuracy_rf(X2.detach(), y.detach(), 300))\n",
    "#print(\"Accuracy with dataset2\", compute_accuracy_rf(dataset2, y, 1000))\n",
    "#print(\"Accuracy with dataset1 + dataset2\", compute_accuracy_rf(torch.cat((dataset1, dataset2), dim=1), y, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Random Forest --\n",
      "Accuracy with dataset1 0.5033684210526316\n",
      "Accuracy with dataset2 0.4947894736842105\n"
     ]
    }
   ],
   "source": [
    "# match indices of dataset1 and dataset2 on the first coordinate\n",
    "distance = torch.cdist(dataset1[:, 0].reshape(-1, 1), dataset2[:, 0].reshape(-1, 1))\n",
    "# replace the 2nd coordinate of dataset1 by the 2nd coordinate of dataset2\n",
    "dataset1_copy = dataset1.clone()\n",
    "dataset1_copy[:, 1] = torch.sum(dataset2[distance.argmin(dim=1)], dim=1)\n",
    "\n",
    "\n",
    "print(\"-- Random Forest --\")\n",
    "print(\"Accuracy with dataset1\", compute_accuracy_rf(dataset1_copy, y, 1000))\n",
    "print(\"Accuracy with dataset2\", compute_accuracy_rf(dataset2, y, 1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- TabPFN original --\n",
      "interface\n",
      "torch.Size([10000, 1, 10])\n",
      "torch.Size([10000, 1])\n",
      "1000\n",
      "Accuracy with dataset1 0.9217777777777778\n"
     ]
    }
   ],
   "source": [
    "print(\"-- TabPFN original --\")\n",
    "print(\"Accuracy with dataset1\", compute_accuracy_tabpfn_original(dataset1, y, 1000))\n",
    "#print(\"Accuracy with dataset2\", compute_accuracy_tabpfn_original(dataset2, y, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- TabPFN --\n",
      "Without preprocessing\n",
      " ** On entry to SGEMM  parameter number 10 had an illegal value\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m-- TabPFN --\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mWithout preprocessing\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAccuracy with dataset1\u001b[39m\u001b[39m\"\u001b[39m, compute_accuracy_tabpfn(dataset1, y, \u001b[39m1000\u001b[39;49m, preprocess\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m))\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAccuracy with dataset2\u001b[39m\u001b[39m\"\u001b[39m, compute_accuracy_tabpfn(dataset2, y, \u001b[39m1000\u001b[39m, preprocess\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m))\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mWith preprocessing\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[32], line 29\u001b[0m, in \u001b[0;36mcompute_accuracy_tabpfn\u001b[0;34m(X, y, n_train, preprocess)\u001b[0m\n\u001b[1;32m     27\u001b[0m     X \u001b[39m=\u001b[39m preprocess_input(X, y, [], \u001b[39m'\u001b[39m\u001b[39mpower\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     28\u001b[0m X \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 29\u001b[0m y_pred \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(module((X, y), single_eval_pos\u001b[39m=\u001b[39;49mn_train, x_already_encoded\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[39m# Check the accuracy\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39mprint\u001b[39m(accuracy_score(y[\u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(y_pred):]\u001b[39m.\u001b[39mcpu(), y_pred\u001b[39m.\u001b[39mcpu()))\n",
      "File \u001b[0;32m~/.local/miniconda3/envs/tab_pfn2/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/storage/store/work/lgrinszt/TabPFN/tabpfn/transformer.py:120\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[0;34m(self, src, src_mask, single_eval_pos, x_already_encoded)\u001b[0m\n\u001b[1;32m    118\u001b[0m style_src, x_src, y_src \u001b[39m=\u001b[39m src\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m x_already_encoded:\n\u001b[0;32m--> 120\u001b[0m     x_src \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x_src)\n\u001b[1;32m    121\u001b[0m y_src \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_encoder(y_src\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(y_src\u001b[39m.\u001b[39mshape) \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(x_src\u001b[39m.\u001b[39mshape) \u001b[39melse\u001b[39;00m y_src)\n\u001b[1;32m    122\u001b[0m style_src \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstyle_encoder(style_src)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstyle_encoder \u001b[39melse\u001b[39;00m \\\n\u001b[1;32m    123\u001b[0m     torch\u001b[39m.\u001b[39mtensor([], device\u001b[39m=\u001b[39mx_src\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/.local/miniconda3/envs/tab_pfn2/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/storage/store/work/lgrinszt/TabPFN/tabpfn/encoders.py:199\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplace_nan_by_zero:\n\u001b[1;32m    198\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnan_to_num(x, nan\u001b[39m=\u001b[39m\u001b[39m0.0\u001b[39m)\n\u001b[0;32m--> 199\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mforward(x)\n",
      "File \u001b[0;32m~/.local/miniconda3/envs/tab_pfn2/lib/python3.9/site-packages/torch/nn/modules/linear.py:96\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/.local/miniconda3/envs/tab_pfn2/lib/python3.9/site-packages/torch/nn/functional.py:1847\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight):\n\u001b[1;32m   1846\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[39minput\u001b[39m, weight), \u001b[39minput\u001b[39m, weight, bias\u001b[39m=\u001b[39mbias)\n\u001b[0;32m-> 1847\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, weight, bias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`"
     ]
    }
   ],
   "source": [
    "print(\"-- TabPFN --\")\n",
    "print(\"Without preprocessing\")\n",
    "print(\"Accuracy with dataset1\", compute_accuracy_tabpfn(dataset1, y, 1000, preprocess=False))\n",
    "print(\"Accuracy with dataset2\", compute_accuracy_tabpfn(dataset2, y, 1000, preprocess=False))\n",
    "print(\"With preprocessing\")\n",
    "print(\"Accuracy with dataset1\", compute_accuracy_tabpfn(dataset1, y, 1000, preprocess=True))\n",
    "print(\"Accuracy with dataset2\", compute_accuracy_tabpfn(dataset2, y, 1000, preprocess=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num classes 10\n",
      "torch.Size([10000, 10])\n",
      "torch.Size([10000])\n",
      "Epoch 0\n",
      "----------------\n",
      "Train loss 2.2130115372794017\n",
      "Train accuracy tensor(0.2254, device='cuda:2')\n",
      "---------------\n",
      "Test loss 2.1438475449879966\n",
      "Test accuracy tensor(0.3096, device='cuda:2')\n",
      "---------------\n",
      "Epoch 1\n",
      "----------------\n",
      "Train loss 2.08979537657329\n",
      "Train accuracy tensor(0.3722, device='cuda:2')\n",
      "---------------\n",
      "Test loss 2.05389674504598\n",
      "Test accuracy tensor(0.4228, device='cuda:2')\n",
      "---------------\n",
      "Epoch 2\n",
      "----------------\n",
      "Train loss 1.9747475002493178\n",
      "Train accuracy tensor(0.5422, device='cuda:2')\n",
      "---------------\n",
      "Test loss 1.966934899489085\n",
      "Test accuracy tensor(0.5334, device='cuda:2')\n",
      "---------------\n",
      "Epoch 3\n",
      "----------------\n",
      "Train loss 1.8911586701869965\n",
      "Train accuracy tensor(0.6330, device='cuda:2')\n",
      "---------------\n",
      "Test loss 1.8783376117547352\n",
      "Test accuracy tensor(0.6468, device='cuda:2')\n",
      "---------------\n",
      "Epoch 4\n",
      "----------------\n",
      "Train loss 1.8223920634814672\n",
      "Train accuracy tensor(0.7068, device='cuda:2')\n",
      "---------------\n",
      "Test loss 1.8326589564482372\n",
      "Test accuracy tensor(0.6987, device='cuda:2')\n",
      "---------------\n",
      "Epoch 5\n",
      "----------------\n",
      "Train loss 1.7942883116858346\n",
      "Train accuracy tensor(0.7215, device='cuda:2')\n",
      "---------------\n",
      "Test loss 1.8101242085297902\n",
      "Test accuracy tensor(0.6881, device='cuda:2')\n",
      "---------------\n",
      "Epoch 6\n",
      "----------------\n",
      "Train loss 1.7610415603433336\n",
      "Train accuracy tensor(0.7454, device='cuda:2')\n",
      "---------------\n",
      "Test loss 1.7883126338322957\n",
      "Test accuracy tensor(0.7140, device='cuda:2')\n",
      "---------------\n",
      "Epoch 7\n",
      "----------------\n",
      "Train loss 1.7455605736800603\n",
      "Train accuracy tensor(0.7559, device='cuda:2')\n",
      "---------------\n",
      "Test loss 1.7821609278519948\n",
      "Test accuracy tensor(0.7178, device='cuda:2')\n",
      "---------------\n",
      "Epoch 8\n",
      "----------------\n",
      "Train loss 1.7215235701629095\n",
      "Train accuracy tensor(0.7760, device='cuda:2')\n",
      "---------------\n",
      "Test loss 1.7576465010643005\n",
      "Test accuracy tensor(0.7404, device='cuda:2')\n",
      "---------------\n",
      "Epoch 9\n",
      "----------------\n",
      "Train loss 1.7073794603347778\n",
      "Train accuracy tensor(0.7906, device='cuda:2')\n",
      "---------------\n",
      "Test loss 1.745889574289322\n",
      "Test accuracy tensor(0.7507, device='cuda:2')\n",
      "---------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(CustomModel(\n",
       "   (scorer): PairwiseMLP(\n",
       "     (fc1): Linear(in_features=20, out_features=256, bias=True)\n",
       "     (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (fc3): Linear(in_features=256, out_features=1, bias=True)\n",
       "   )\n",
       " ),\n",
       " MLP(\n",
       "   (fc1): Linear(in_features=20, out_features=256, bias=True)\n",
       "   (relu): ReLU()\n",
       "   (fc2): Linear(in_features=256, out_features=10, bias=True)\n",
       " ))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_attention(module, dataset1, dataset2, y, 1000, preprocess=True, lr=0.001, n_epochs=1000, train_tabpfn=False)\n",
    "train_matching(dataset1, dataset2, y, 256, preprocess=True, lr=0.001, n_epochs=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "{'recompute_attn': True}\n",
      "<module 'wandb' from '/home/parietal/lgrinszt/.local/miniconda3/envs/tab_pfn2/lib/python3.9/site-packages/wandb/__init__.py'>\n",
      "Using cuda:0 device\n",
      "Batch size: 1\n",
      "Using distributed training: False\n",
      "Using a Transformer with 25.82 M parameters\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\"\n",
    "model = TabPFNClassifier(device=device)\n",
    "\n",
    "module = model.model[2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## California"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the same thing on the california housing dataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "\n",
    "data = fetch_california_housing(as_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_california = data[\"frame\"]\n",
    "# drop the target\n",
    "df_california = df_california.drop(\"MedHouseVal\", axis=1)\n",
    "# drop income\n",
    "df_california_without_income = df_california.drop(\"MedInc\", axis=1)\n",
    "# Drop more columns to make it harder\n",
    "# df_california = df_california.drop(\"AveOccup\", axis=1)\n",
    "# df_california_without_income = df_california_without_income.drop(\"AveOccup\", axis=1)\n",
    "# only keep Latitude and Longitude\n",
    "df_california = df_california[[\"MedInc\", \"Latitude\", \"Longitude\"]]\n",
    "df_california_without_income = df_california_without_income[[\"Latitude\", \"Longitude\"]]\n",
    "target = data[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20635</th>\n",
       "      <td>1.5603</td>\n",
       "      <td>39.48</td>\n",
       "      <td>-121.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20636</th>\n",
       "      <td>2.5568</td>\n",
       "      <td>39.49</td>\n",
       "      <td>-121.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20637</th>\n",
       "      <td>1.7000</td>\n",
       "      <td>39.43</td>\n",
       "      <td>-121.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20638</th>\n",
       "      <td>1.8672</td>\n",
       "      <td>39.43</td>\n",
       "      <td>-121.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20639</th>\n",
       "      <td>2.3886</td>\n",
       "      <td>39.37</td>\n",
       "      <td>-121.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20640 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       MedInc  Latitude  Longitude\n",
       "0      8.3252     37.88    -122.23\n",
       "1      8.3014     37.86    -122.22\n",
       "2      7.2574     37.85    -122.24\n",
       "3      5.6431     37.85    -122.25\n",
       "4      3.8462     37.85    -122.25\n",
       "...       ...       ...        ...\n",
       "20635  1.5603     39.48    -121.09\n",
       "20636  2.5568     39.49    -121.21\n",
       "20637  1.7000     39.43    -121.22\n",
       "20638  1.8672     39.43    -121.32\n",
       "20639  2.3886     39.37    -121.24\n",
       "\n",
       "[20640 rows x 3 columns]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_california"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'City': ['Los Angeles', 'San Diego', 'San Jose', 'San Francisco', 'Fresno', 'Sacramento', 'Long Beach', 'Oakland', 'Bakersfield', 'Anaheim', 'Santa Ana', 'Riverside', 'Stockton', 'Chula Vista', 'Irvine', 'Fremont', 'San Bernardino', 'Modesto', 'Fontana', 'Oxnard'], \n",
    "        'Average Household Income': ['$101,006', '$113,681', '$150,601', '$167,663', '$73,396', '$87,213', '$89,912', '$116,585', '$84,592', '$97,136', '$88,829', '$90,520', '$78,712', '$105,155', '$140,764', '$170,083', '$64,929', '$81,841', '$93,383', '$91,636']}\n",
    "\n",
    "df_cities = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get lat and long for each city\n",
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim(user_agent=\"my-application (leo.grinsztajn@gmail.com)\")\n",
    "df_cities[\"location\"] = df_cities[\"City\"].apply(lambda x: geolocator.geocode(x))\n",
    "df_cities[\"latitude\"] = df_cities[\"location\"].apply(lambda x: x.latitude)\n",
    "df_cities[\"longitude\"] = df_cities[\"location\"].apply(lambda x: x.longitude)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Convert target to a classification problem\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m target_classif \u001b[39m=\u001b[39m (target \u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mmedian(target))\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Convert df_california to a torch tensor\u001b[39;00m\n\u001b[1;32m      5\u001b[0m california_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(df_california\u001b[39m.\u001b[39mvalues)\u001b[39m.\u001b[39mfloat()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'target' is not defined"
     ]
    }
   ],
   "source": [
    "# Convert target to a classification problem\n",
    "target_classif = (target > np.median(target)).astype(int)\n",
    "\n",
    "# Convert df_california to a torch tensor\n",
    "california_tensor = torch.tensor(df_california.values).float()\n",
    "california_without_income_tensor = torch.tensor(df_california_without_income.values).float()\n",
    "target_tensor = torch.tensor(target_classif.values).float()\n",
    "\n",
    "# Suffle the data\n",
    "from sklearn.utils import shuffle\n",
    "california_tensor, california_without_income_tensor, target_tensor = shuffle(california_tensor, california_without_income_tensor, target_tensor, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape\n",
    "california_tensor = california_tensor.reshape(california_tensor.shape[0], 1, california_tensor.shape[1])\n",
    "california_without_income_tensor = california_without_income_tensor.reshape(california_without_income_tensor.shape[0], 1, california_without_income_tensor.shape[1])\n",
    "target_tensor = target_tensor.reshape(target.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 1.], dtype=float32), array([2549, 2451]))\n",
      "0.4902\n",
      "0.5098\n"
     ]
    }
   ],
   "source": [
    "# Truncate the first dim to 5000\n",
    "california_tensor = california_tensor[:5000]\n",
    "california_without_income_tensor = california_without_income_tensor[:5000]\n",
    "target_tensor = target_tensor[:5000]\n",
    "\n",
    "print(np.unique(target_tensor, return_counts=True))\n",
    "# print proportion of 1\n",
    "p = np.unique(target_tensor, return_counts=True)[1][1] / np.unique(target_tensor, return_counts=True)[1].sum()\n",
    "print(p)\n",
    "print(1-p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 1, 3])\n",
      "torch.Size([5000, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "print(california_tensor.shape)\n",
    "print(california_without_income_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to GPU\n",
    "california_tensor = california_tensor.to(device)\n",
    "california_without_income_tensor = california_without_income_tensor.to(device)\n",
    "target_tensor = target_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interface\n",
      "torch.Size([5000, 1, 3])\n",
      "torch.Size([5000, 1])\n",
      "1000\n",
      "src_mask None\n",
      "Accuracy with income\n",
      "0.8215\n",
      "interface\n",
      "torch.Size([5000, 1, 2])\n",
      "torch.Size([5000, 1])\n",
      "1000\n",
      "src_mask None\n",
      "Accuracy without income\n",
      "0.77825\n"
     ]
    }
   ],
   "source": [
    "#model = TabPFNClassifier(N_ensemble_configurations=1, feature_shift_decoder=False)\n",
    "\n",
    "#module = model.model[2]\n",
    "# Pad with 0s until we have 100 features\n",
    "\n",
    "n_samples, n_features = california_tensor.shape[0], california_tensor.shape[2]\n",
    "model.fit(california_tensor[:1000].reshape(1000, n_features).cpu(), target_tensor[:1000].reshape(1000).cpu())\n",
    "y_pred = model.predict(california_tensor[1000:].reshape(4000, n_features).cpu())\n",
    "print(\"Accuracy with income\")\n",
    "print(accuracy_score(target_tensor[1000:].reshape(-1).cpu(), y_pred))\n",
    "\n",
    "n_samples, n_features = california_without_income_tensor.shape[0], california_without_income_tensor.shape[2]\n",
    "model.fit(california_without_income_tensor[:1000].reshape(1000, n_features).cpu(), target_tensor[:1000].reshape(1000).cpu())\n",
    "y_pred = model.predict(california_without_income_tensor[1000:].reshape(4000, n_features).cpu())\n",
    "print(\"Accuracy without income\")\n",
    "print(accuracy_score(target_tensor[1000:].reshape(-1).cpu(), y_pred))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 1, 3])\n",
      "torch.Size([5000, 1, 100])\n",
      "src_mask None\n",
      "src_mask None\n",
      "torch.Size([4000])\n",
      "torch.Size([4000])\n",
      "Accuracy with income\n",
      "0.822\n",
      "Accuracy without income\n",
      "0.783\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, QuantileTransformer\n",
    "#TODO why the difference with random forest?\n",
    "#scaler = StandardScaler()\n",
    "#california_tensor_processed = torch.tensor(scaler.fit_transform(california_tensor.reshape(-1, california_tensor.shape[-1])).reshape(california_tensor.shape))\n",
    "#california_without_income_tensor_processed = torch.tensor(scaler.fit_transform(california_without_income_tensor.reshape(-1, california_without_income_tensor.shape[-1])).reshape(california_without_income_tensor.shape))\n",
    "print(california_tensor.shape)\n",
    "california_tensor_processed = preprocess_input(california_tensor, target_tensor, [], preprocess_transform='power')\n",
    "california_without_income_tensor_processed = preprocess_input(california_without_income_tensor, target_tensor, [], preprocess_transform='power')\n",
    "# Pad last dimension with 0 to 100\n",
    "california_tensor_processed = torch.nn.functional.pad(california_tensor_processed, (0, 100-california_tensor.shape[2]))\n",
    "california_without_income_tensor_processed = torch.nn.functional.pad(california_without_income_tensor_processed, (0, 100-california_without_income_tensor.shape[2]))\n",
    "print(california_tensor_processed.shape)\n",
    "# convert to float\n",
    "california_tensor_processed = california_tensor_processed.float()\n",
    "y_pred = torch.argmax(module((california_tensor_processed, target_tensor), single_eval_pos=1000, x_already_encoded=False), dim=-1).reshape(-1)\n",
    "california_without_income_tensor_processed = california_without_income_tensor_processed.float()\n",
    "y_pred2 = torch.argmax(module((california_without_income_tensor_processed, target_tensor), single_eval_pos=1000, x_already_encoded=False), dim=-1).reshape(-1)\n",
    "# print shapes\n",
    "print(y_pred.shape)\n",
    "print(y_pred2.shape)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy with income\")\n",
    "print(accuracy_score(target_tensor[-len(y_pred):].cpu(), y_pred.cpu()))\n",
    "print(\"Accuracy without income\")\n",
    "print(accuracy_score(target_tensor[-len(y_pred2):].cpu(), y_pred2.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with income\n",
      "0.84425\n",
      "Accuracy without income\n",
      "0.835\n"
     ]
    }
   ],
   "source": [
    "# Try with a random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "X, y = california_tensor.reshape(california_tensor.shape[0], -1).cpu().numpy(), target_tensor.reshape(-1).cpu().numpy()\n",
    "# The train part is the first 1000 samples\n",
    "rf.fit(X[:1000], y[:1000])\n",
    "y_pred = rf.predict(X[1000:])\n",
    "print(\"Accuracy with income\")\n",
    "print(accuracy_score(y[1000:], y_pred))\n",
    "\n",
    "# without income\n",
    "rf = RandomForestClassifier()\n",
    "X, y = california_without_income_tensor.reshape(california_without_income_tensor.shape[0], -1).cpu().numpy(), target_tensor.reshape(-1).cpu().numpy()\n",
    "# The train part is the first 1000 samples\n",
    "rf.fit(X[:1000], y[:1000])\n",
    "y_pred = rf.predict(X[1000:])\n",
    "print(\"Accuracy without income\")\n",
    "print(accuracy_score(y[1000:], y_pred))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Average Household Income</th>\n",
       "      <th>location</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>$101,006</td>\n",
       "      <td>(Los Angeles, Los Angeles County, CAL Fire Con...</td>\n",
       "      <td>34.053691</td>\n",
       "      <td>-118.242766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>San Diego</td>\n",
       "      <td>$113,681</td>\n",
       "      <td>(San Diego, San Diego County, CAL Fire Souther...</td>\n",
       "      <td>32.717420</td>\n",
       "      <td>-117.162773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>San Jose</td>\n",
       "      <td>$150,601</td>\n",
       "      <td>(San Jose, Santa Clara County, CAL Fire Northe...</td>\n",
       "      <td>37.336166</td>\n",
       "      <td>-121.890591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>San Francisco</td>\n",
       "      <td>$167,663</td>\n",
       "      <td>(San Francisco, CAL Fire Northern Region, Cali...</td>\n",
       "      <td>37.779026</td>\n",
       "      <td>-122.419906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fresno</td>\n",
       "      <td>$73,396</td>\n",
       "      <td>(Fresno, Fresno County, CAL Fire Southern Regi...</td>\n",
       "      <td>36.739442</td>\n",
       "      <td>-119.784831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sacramento</td>\n",
       "      <td>$87,213</td>\n",
       "      <td>(Sacramento, Sacramento County, CAL Fire North...</td>\n",
       "      <td>38.581061</td>\n",
       "      <td>-121.493895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Long Beach</td>\n",
       "      <td>$89,912</td>\n",
       "      <td>(Long Beach, Los Angeles County, CAL Fire Cont...</td>\n",
       "      <td>33.769016</td>\n",
       "      <td>-118.191604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Oakland</td>\n",
       "      <td>$116,585</td>\n",
       "      <td>(Oakland, Alameda County, CAL Fire Northern Re...</td>\n",
       "      <td>37.804456</td>\n",
       "      <td>-122.271356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bakersfield</td>\n",
       "      <td>$84,592</td>\n",
       "      <td>(Bakersfield, Kern County, CAL Fire Southern R...</td>\n",
       "      <td>35.373871</td>\n",
       "      <td>-119.019464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Anaheim</td>\n",
       "      <td>$97,136</td>\n",
       "      <td>(Anaheim, Orange County, CAL Fire Contract Cou...</td>\n",
       "      <td>33.834752</td>\n",
       "      <td>-117.911732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Santa Ana</td>\n",
       "      <td>$88,829</td>\n",
       "      <td>(Santa Ana, Orange County, CAL Fire Southern R...</td>\n",
       "      <td>33.749495</td>\n",
       "      <td>-117.873221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Riverside</td>\n",
       "      <td>$90,520</td>\n",
       "      <td>(Riverside, Riverside County, California, Unit...</td>\n",
       "      <td>33.953355</td>\n",
       "      <td>-117.396162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Stockton</td>\n",
       "      <td>$78,712</td>\n",
       "      <td>(Stockton, San Joaquin County, CAL Fire Southe...</td>\n",
       "      <td>37.957702</td>\n",
       "      <td>-121.290780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Chula Vista</td>\n",
       "      <td>$105,155</td>\n",
       "      <td>(Chula Vista, San Diego County, California, Un...</td>\n",
       "      <td>32.640054</td>\n",
       "      <td>-117.084196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Irvine</td>\n",
       "      <td>$140,764</td>\n",
       "      <td>(Irvine, Orange County, CAL Fire Southern Regi...</td>\n",
       "      <td>33.685697</td>\n",
       "      <td>-117.825982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Fremont</td>\n",
       "      <td>$170,083</td>\n",
       "      <td>(Fremont, Alameda County, CAL Fire Northern Re...</td>\n",
       "      <td>37.548270</td>\n",
       "      <td>-121.988571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>San Bernardino</td>\n",
       "      <td>$64,929</td>\n",
       "      <td>(San Bernardino County, CAL Fire Southern Regi...</td>\n",
       "      <td>34.825302</td>\n",
       "      <td>-116.083314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Modesto</td>\n",
       "      <td>$81,841</td>\n",
       "      <td>(Modesto, Stanislaus County, CAL Fire Southern...</td>\n",
       "      <td>37.639097</td>\n",
       "      <td>-120.996878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Fontana</td>\n",
       "      <td>$93,383</td>\n",
       "      <td>(Fontana, San Bernardino County, CAL Fire Sout...</td>\n",
       "      <td>34.092233</td>\n",
       "      <td>-117.435048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Oxnard</td>\n",
       "      <td>$91,636</td>\n",
       "      <td>(Oxnard, Ventura County, CAL Fire Contract Cou...</td>\n",
       "      <td>34.197631</td>\n",
       "      <td>-119.180382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              City Average Household Income  \\\n",
       "0      Los Angeles                 $101,006   \n",
       "1        San Diego                 $113,681   \n",
       "2         San Jose                 $150,601   \n",
       "3    San Francisco                 $167,663   \n",
       "4           Fresno                  $73,396   \n",
       "5       Sacramento                  $87,213   \n",
       "6       Long Beach                  $89,912   \n",
       "7          Oakland                 $116,585   \n",
       "8      Bakersfield                  $84,592   \n",
       "9          Anaheim                  $97,136   \n",
       "10       Santa Ana                  $88,829   \n",
       "11       Riverside                  $90,520   \n",
       "12        Stockton                  $78,712   \n",
       "13     Chula Vista                 $105,155   \n",
       "14          Irvine                 $140,764   \n",
       "15         Fremont                 $170,083   \n",
       "16  San Bernardino                  $64,929   \n",
       "17         Modesto                  $81,841   \n",
       "18         Fontana                  $93,383   \n",
       "19          Oxnard                  $91,636   \n",
       "\n",
       "                                             location   latitude   longitude  \n",
       "0   (Los Angeles, Los Angeles County, CAL Fire Con...  34.053691 -118.242766  \n",
       "1   (San Diego, San Diego County, CAL Fire Souther...  32.717420 -117.162773  \n",
       "2   (San Jose, Santa Clara County, CAL Fire Northe...  37.336166 -121.890591  \n",
       "3   (San Francisco, CAL Fire Northern Region, Cali...  37.779026 -122.419906  \n",
       "4   (Fresno, Fresno County, CAL Fire Southern Regi...  36.739442 -119.784831  \n",
       "5   (Sacramento, Sacramento County, CAL Fire North...  38.581061 -121.493895  \n",
       "6   (Long Beach, Los Angeles County, CAL Fire Cont...  33.769016 -118.191604  \n",
       "7   (Oakland, Alameda County, CAL Fire Northern Re...  37.804456 -122.271356  \n",
       "8   (Bakersfield, Kern County, CAL Fire Southern R...  35.373871 -119.019464  \n",
       "9   (Anaheim, Orange County, CAL Fire Contract Cou...  33.834752 -117.911732  \n",
       "10  (Santa Ana, Orange County, CAL Fire Southern R...  33.749495 -117.873221  \n",
       "11  (Riverside, Riverside County, California, Unit...  33.953355 -117.396162  \n",
       "12  (Stockton, San Joaquin County, CAL Fire Southe...  37.957702 -121.290780  \n",
       "13  (Chula Vista, San Diego County, California, Un...  32.640054 -117.084196  \n",
       "14  (Irvine, Orange County, CAL Fire Southern Regi...  33.685697 -117.825982  \n",
       "15  (Fremont, Alameda County, CAL Fire Northern Re...  37.548270 -121.988571  \n",
       "16  (San Bernardino County, CAL Fire Southern Regi...  34.825302 -116.083314  \n",
       "17  (Modesto, Stanislaus County, CAL Fire Southern...  37.639097 -120.996878  \n",
       "18  (Fontana, San Bernardino County, CAL Fire Sout...  34.092233 -117.435048  \n",
       "19  (Oxnard, Ventura County, CAL Fire Contract Cou...  34.197631 -119.180382  "
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert value like $101,006' to 101006\n",
    "df_cities[\"Average Household Income\"] = df_cities[\"Average Household Income\"].apply(lambda x: int(x.replace(\"$\", \"\").replace(\",\", \"\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Average Household Income</th>\n",
       "      <th>location</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>101006</td>\n",
       "      <td>(Los Angeles, Los Angeles County, CAL Fire Con...</td>\n",
       "      <td>34.053691</td>\n",
       "      <td>-118.242766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>San Diego</td>\n",
       "      <td>113681</td>\n",
       "      <td>(San Diego, San Diego County, CAL Fire Souther...</td>\n",
       "      <td>32.717420</td>\n",
       "      <td>-117.162773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>San Jose</td>\n",
       "      <td>150601</td>\n",
       "      <td>(San Jose, Santa Clara County, CAL Fire Northe...</td>\n",
       "      <td>37.336166</td>\n",
       "      <td>-121.890591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>San Francisco</td>\n",
       "      <td>167663</td>\n",
       "      <td>(San Francisco, CAL Fire Northern Region, Cali...</td>\n",
       "      <td>37.779026</td>\n",
       "      <td>-122.419906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fresno</td>\n",
       "      <td>73396</td>\n",
       "      <td>(Fresno, Fresno County, CAL Fire Southern Regi...</td>\n",
       "      <td>36.739442</td>\n",
       "      <td>-119.784831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            City  Average Household Income  \\\n",
       "0    Los Angeles                    101006   \n",
       "1      San Diego                    113681   \n",
       "2       San Jose                    150601   \n",
       "3  San Francisco                    167663   \n",
       "4         Fresno                     73396   \n",
       "\n",
       "                                            location   latitude   longitude  \n",
       "0  (Los Angeles, Los Angeles County, CAL Fire Con...  34.053691 -118.242766  \n",
       "1  (San Diego, San Diego County, CAL Fire Souther...  32.717420 -117.162773  \n",
       "2  (San Jose, Santa Clara County, CAL Fire Northe...  37.336166 -121.890591  \n",
       "3  (San Francisco, CAL Fire Northern Region, Cali...  37.779026 -122.419906  \n",
       "4  (Fresno, Fresno County, CAL Fire Southern Regi...  36.739442 -119.784831  "
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert df_cities to a torch tensor\n",
    "cities_tensor = torch.tensor(df_cities[[\"Average Household Income\", \"latitude\", \"longitude\"]].values).float()\n",
    "# Reshape\n",
    "cities_tensor = cities_tensor.reshape(cities_tensor.shape[0], 1, cities_tensor.shape[1])\n",
    "\n",
    "# preporcess the data\n",
    "#cities_tensor_processed = preprocess_input(cities_tensor, target_tensor, [], preprocess_transform='power')\n",
    "#cities_tensor_processed = cities_tensor\n",
    "# Use a standard scaler\n",
    "scaler = StandardScaler()\n",
    "cities_tensor_processed = torch.tensor(scaler.fit_transform(cities_tensor.reshape(-1, cities_tensor.shape[-1])).reshape(cities_tensor.shape))\n",
    "# convert to float\n",
    "cities_tensor_processed = cities_tensor_processed.float()\n",
    "\n",
    "# Pad last dimension with 0 to 100\n",
    "#TODO: think, do I need to do this or can I just put the vector in the attention?\n",
    "cities_tensor_processed = torch.nn.functional.pad(cities_tensor_processed, (0, 100-cities_tensor.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move to GPU\n",
    "cities_tensor_processed = cities_tensor_processed.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import multihead attention\n",
    "from torch.nn import MultiheadAttention\n",
    "n = 512\n",
    "attention = MultiheadAttention(embed_dim=n, num_heads=1, dropout=0.0, device=device)\n",
    "W_q = torch.rand(n, n).to(device)\n",
    "W_k = torch.rand(n, n).to(device)\n",
    "W_v = torch.rand(n, n).to(device)\n",
    "W_q.requires_grad = True\n",
    "W_k.requires_grad = True\n",
    "W_v.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 1000\n",
    "val_size = 2000\n",
    "test_size = 2000\n",
    "\n",
    "assert train_size + val_size + test_size == len(california_tensor_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suffle the \"Average Household Income\" column\n",
    "cities_tensor_processed_random = cities_tensor_processed[torch.randperm(cities_tensor_processed.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_mask None\n",
      "tensor(0.4882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "0.7615\n",
      "src_mask None\n",
      "tensor(0.4905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.4689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.4556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.4462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.4401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.4395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.4360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.4312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.4311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.4279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "0.804\n",
      "src_mask None\n",
      "tensor(0.4248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.4286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.4233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.4207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.4206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.4182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.4146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.4167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.4140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.4106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "0.8145\n",
      "src_mask None\n",
      "tensor(0.4094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.4084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.4063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.4033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.4027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.4087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.4043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.4033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.4005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "0.815\n",
      "src_mask None\n",
      "tensor(0.3970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "0.816\n",
      "src_mask None\n",
      "tensor(0.3895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "0.816\n",
      "src_mask None\n",
      "tensor(0.3825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "0.814\n",
      "src_mask None\n",
      "tensor(0.3794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "0.817\n",
      "src_mask None\n",
      "tensor(0.3761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "0.8205\n",
      "src_mask None\n",
      "tensor(0.3744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "0.822\n",
      "src_mask None\n",
      "tensor(0.3737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "src_mask None\n",
      "tensor(0.3729, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# optimize the attention matrix W_q, W_k, W_v\n",
    "#TODO: check if we train on test\n",
    "from torch import optim\n",
    "optimizer = optim.Adam([W_q, W_k, W_v], lr=0.01)\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    input = model.model[2].encoder(california_without_income_tensor_processed)\n",
    "    input2 = model.model[2].encoder(cities_tensor_processed)\n",
    "    Q = torch.matmul(input, W_q)\n",
    "    K = torch.matmul(input2, W_k)\n",
    "    V = torch.matmul(input2, W_v)\n",
    "    full_input = input + attention(Q, K, V)[0]\n",
    "    y_pred = module((full_input, target_tensor), single_eval_pos=train_size, x_already_encoded=True).squeeze()\n",
    "    loss = torch.nn.functional.cross_entropy(y_pred[:val_size], target_tensor[-len(y_pred):-len(y_pred) + val_size].squeeze().long())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss)\n",
    "    if i % 10 == 0:\n",
    "        y_pred = torch.argmax(y_pred, dim=-1).reshape(-1)\n",
    "        print(accuracy_score(target_tensor[-len(y_pred) + val_size:].cpu(), y_pred[val_size:].cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: GPU and correct scoring of baselinees"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tab_pfn2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "21dbb7094b1385f0bae0b91ae49063169e8dea4181459eda714e1f1fb7500475"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
